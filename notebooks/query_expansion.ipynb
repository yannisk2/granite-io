{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacb7119",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite query expansion intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite Query Expansion (QE) intrinsic. Given a conversation ending with a user query, QE is specifically designed to probe the retriever from multiple angles by generating a set of semantically diverse versions of that last user query.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the models on your own server. To use your own server, set the `run_server` variable below to `False` and set appropriate values for the constants `openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "from granite_io.io.query_expansion import QueryExpansionIOProcessor\n",
    "from granite_io.io.query_rewrite import QueryRewriteIOProcessor\n",
    "\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.base import ChatCompletionInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ceb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "lora_model_name = \"ibm-granite/granite-3.2-8b-lora-rag-query-rewrite\"\n",
    "run_server = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed046ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_name)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    query_rewrite_lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    query_rewrite_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf88f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Welcome to the California State Parks help desk.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm a student. Do you have internships?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The California State Parks hires Student Assistants \"\n",
    "        \"to perform a variety of tasks that require limited or no previous \"\n",
    "        \"work experience.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Cool, how do I sign up?\"},\n",
    "]\n",
    "\n",
    "chat_input_tmp = ChatCompletionInputs(\n",
    "    messages=input_messages,\n",
    "    generate_inputs={\n",
    "        \"temperature\": 1,\n",
    "        \"max_tokens\": 4096,\n",
    "    },\n",
    ")\n",
    "print(\"Inputs for chat completion:\", chat_input_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49241680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an IO processor for the base model\n",
    "io_processor = make_io_processor(base_model_name, backend=backend)\n",
    "rewrite_io_proc = QueryRewriteIOProcessor(query_rewrite_lora_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467097a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_io_proc = QueryExpansionIOProcessor(\n",
    "    backend,\n",
    "    io_processor,\n",
    "    rewrite_io_proc,\n",
    ")\n",
    "\n",
    "qe_result = rag_io_proc.create_chat_completion(chat_input_tmp)\n",
    "print(qe_result)\n",
    "\n",
    "qe_result_strs = [r.next_message.content for r in qe_result.results]\n",
    "print(\"\\nQuery Expansion Results:\")\n",
    "for i, result in enumerate(qe_result_strs):\n",
    "    print(f\"Result {i + 1}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5a479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite_io_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
