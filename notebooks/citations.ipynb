{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite citations intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite citations intrisic, \n",
    "also known as the [LoRA Adapter for Citation Generation](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-citation-generation\n",
    ")\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants \n",
    "`openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    Granite3Point2Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from IPython.display import display, Markdown\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.citations import CitationsCompositeIOProcessor, CitationsIOProcessor\n",
    "from granite_io.visualization import CitationsWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "lora_model_name = \"ibm-granite/granite-3.2-8b-lora-rag-citation-generation\"\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_name)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = Granite3Point2Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the visibility level of Git Repos and Issue \\\n",
    "Tracking projects?\",\n",
    "            }\n",
    "        ],\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"text\": \"Git Repos and Issue Tracking is an IBM-hosted component of \\\n",
    "the Continuous Delivery service. All of the data that you provide to Git Repos and \\\n",
    "Issue Tracking, including but not limited to source files, issues, pull requests, and \\\n",
    "project configuration properties, is managed securely within Continuous Delivery. \\\n",
    "However, Git Repos and Issue Tracking supports various mechanisms for exporting, \\\n",
    "sending, or otherwise sharing data to users and third parties. The ability of Git \\\n",
    "Repos and Issue Tracking to share information is typical of many social coding \\\n",
    "platforms. However, such sharing might conflict with regulatory controls that \\\n",
    "apply to your business. After you create a project in Git Repos and Issue Tracking, \\\n",
    "but before you entrust any files, issues, records, or other data with the project, \\\n",
    "review the project settings and change any settings that you deem necessary to \\\n",
    "protect your data. Settings to review include visibility levels, email notifications, \\\n",
    "integrations, web hooks, access tokens, deploy tokens, and deploy keys. Project \\\n",
    "visibility levels \\n\\nGit Repos and Issue Tracking projects can have one of the \\\n",
    "following visibility levels: private, internal, or public. * Private projects are \\\n",
    "visible only to project members. This setting is the default visibility level for new \\\n",
    "projects, and is the most secure visibility level for your data. * Internal projects \\\n",
    "are visible to all users that are logged in to IBM Cloud. * Public projects are \\\n",
    "visible to anyone. To limit project access to only project members, complete the \\\n",
    "following steps:\\n\\n\\n\\n1. From the project sidebar, click Settings > General. \\\n",
    "2. On the General Settings page, click Visibility > project features > permissions. \\\n",
    "3. Locate the Project visibility setting. 4. Select Private, if it is not already \\\n",
    "selected. 5. Click Save changes. Project membership \\n\\nGit Repos and Issue Tracking \\\n",
    "is a cloud hosted social coding environment that is available to all Continuous \\\n",
    "Delivery users. If you are a Git Repos and Issue Tracking project Maintainer or Owner, \\\n",
    "you can invite any user and group members to the project. IBM Cloud places no \\\n",
    "restrictions on who you can invite to a project.\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"After you create a project in Git Repos and Issue Tracking, \\\n",
    "but before you entrust any files, issues, records, or other data with the project, \\\n",
    "review the project settings and change any settings that are necessary to protect your \\\n",
    "data. \\\n",
    "Settings to review include visibility levels, email notifications, integrations, web \\\n",
    "hooks, access tokens, deploy tokens, and deploy keys. Project visibility levels \\\n",
    "\\n\\nGit Repos and Issue Tracking projects can have one of the following visibility \\\n",
    "levels: private, internal, or public. * Private projects are visible only to \\\n",
    "project members. This setting is the default visibility level for new projects, and \\\n",
    "is the most secure visibility level for your data. * Internal projects are visible to \\\n",
    "all users that are logged in to IBM Cloud. * Public projects are visible to anyone. \\\n",
    "To limit project access to only project members, complete the following \\\n",
    "steps:\\n\\n\\n\\n1. From the project sidebar, click Settings > General. 2. On the \\\n",
    "General Settings page, click Visibility > project features > permissions. 3. Locate \\\n",
    "the Project visibility setting. 4. Select Private, if it is not already selected. \\\n",
    "5. Click Save changes. Project email settings \\n\\nBy default, Git Repos and Issue \\\n",
    "Tracking notifies project members by way of email about project activities. These \\\n",
    "emails typically include customer-owned data that was provided to Git Repos and Issue \\\n",
    "Tracking by users. For example, if a user posts a comment to an issue, Git Repos and \\\n",
    "Issue Tracking sends an email to all subscribers. The email includes information such \\\n",
    "as a copy of the comment, the user who posted it, and when the comment was posted. \\\n",
    "To turn off all email notifications for your project, complete the following \\\n",
    "steps:\\n\\n\\n\\n1. From the project sidebar, click Settings > General. 2. On the \\\n",
    "**General Settings **page, click Visibility > project features > permissions. \\\n",
    "3. Select the Disable email notifications checkbox. 4. Click Save changes. Project \\\n",
    "integrations and webhooks\"\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0, \"max_tokens\": 1024},\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example input through Granite 3.2 to get an answer\n",
    "granite_io_proc = make_io_processor(\"Granite 3.2\", backend=backend)\n",
    "result = await granite_io_proc.acreate_chat_completion(chat_input)\n",
    "\n",
    "display(Markdown(result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the model's output to the chat\n",
    "next_chat_input = chat_input.with_next_message(result.results[0].next_message)\n",
    "next_chat_input.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the I/O processor for the citations LoRA adapter\n",
    "io_proc = CitationsIOProcessor(lora_backend)\n",
    "\n",
    "# Pass our example input thorugh the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(next_chat_input)\n",
    "\n",
    "next_message = chat_result.results[0].next_message\n",
    "print(next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize citations using citations widget\n",
    "CitationsWidget().show(next_chat_input, chat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also get raw output of the LoRA adapter for debugging\n",
    "next_message._raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with an artifical poor-quality assistant response.\n",
    "from granite_io.types import AssistantMessage\n",
    "\n",
    "chat_result_2 = await io_proc.acreate_chat_completion(\n",
    "    chat_input.with_next_message(\n",
    "        AssistantMessage(\n",
    "            content=\"Git repos are generally only visible in the infrared \"\n",
    "            \"spectrum, due to their natural camouflage. Issue Tracking projects \"\n",
    "            \"are much easier to see; their bright colors warn predators of the \"\n",
    "            \"poisonous technical debt that they secrete.\"\n",
    "        )\n",
    "    ).with_addl_generate_params({\"temperature\": 0.0})\n",
    ")\n",
    "chat_result_2.results[0].next_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composite citations processor that generates a response and adds citations\n",
    "composite_proc = CitationsCompositeIOProcessor(granite_io_proc, lora_backend)\n",
    "\n",
    "# Note that this codes passes in the original chat input, without an assistant response\n",
    "chat_result_4 = await composite_proc.acreate_chat_completion(chat_input)\n",
    "print(chat_result_4.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also ask the composite IO processor to generate multiple completions, in\n",
    "# which case it will generate citations for all completions in parallel.\n",
    "chat_result_5 = await composite_proc.acreate_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 5, \"temperature\": 0.7})\n",
    ")\n",
    "# print(chat_result_5.model_dump_json(indent=2))\n",
    "\n",
    "for result in chat_result_5.results:\n",
    "    print(f\"Assistant: {result.next_message.content}\")\n",
    "    print(f\"           ({len(result.next_message.citations)} citations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
