{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of retrieval IO processor with MTRAG benchmark data\n",
    "\n",
    "This notebook shows how to use the retrieval IO processor to implement the retrieval \n",
    "phase of Retrieval-Augmented Generation (RAG) on top of Granite 3.2.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "model on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    Granite3Point2Inputs,\n",
    "    ControlsRecord,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.retrieval import InMemoryRetriever, RetrievalRequestProcessor\n",
    "from granite_io.io.retrieval.util import download_mtrag_embeddings\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "corpus_name = \"govt\"\n",
    "embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embed.parquet\"\n",
    "embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "\n",
    "run_server = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(model_name)\n",
    "    server.wait_for_startup(200)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the indexed corpus if it hasn't already been downloaded.\n",
    "# This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "if not os.path.exists(embeddings_location):\n",
    "    download_mtrag_embeddings(embedding_model_name, corpus_name, embeddings_location)\n",
    "embeddings_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an IO processor for the base model\n",
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "io_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completions request\n",
    "chat_input = Granite3Point2Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the California Appellate Courts help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I need to do some legal research to be prepared for my \"\n",
    "                \"oral argument. Can I visit the law library?\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chat completion request through the base model without RAG.\n",
    "# The result should be a refusal message that starts with, \"As an AI, I don't have\n",
    "# physical locations or resources.\"\n",
    "non_rag_result = io_proc.create_chat_completion(chat_input)\n",
    "display(Markdown(non_rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an in-memory vector database\n",
    "retriever = InMemoryRetriever(embeddings_location, embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a RetrievalRequestProcessor to augment the chat completion request with documents.\n",
    "rag_processor = RetrievalRequestProcessor(retriever)\n",
    "rag_chat_input = rag_processor.process(chat_input)[0]\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "print(\"Documents:\")\n",
    "pd.DataFrame.from_records([d.model_dump() for d in rag_chat_input.documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same request through the base model with RAG documents\n",
    "rag_result = io_proc.create_chat_completion(rag_chat_input)\n",
    "display(Markdown(rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat with the base Granite model's native citations support\n",
    "citations_result = io_proc.create_chat_completion(\n",
    "    rag_chat_input.model_copy(update={\"controls\": ControlsRecord(citations=True)})\n",
    ")\n",
    "display(Markdown(citations_result.results[0].next_message.content))\n",
    "\n",
    "print(\"Citations:\")\n",
    "\n",
    "pd.DataFrame.from_records(\n",
    "    [c.model_dump() for c in citations_result.results[0].next_message.citations]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
