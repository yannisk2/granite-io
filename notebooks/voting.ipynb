{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the majority voting IO processor\n",
    "\n",
    "This notebook shows how to use the majority voting IO processor to perform simple\n",
    "majority voting over multiple model responses.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "model on your own server. \n",
    "\n",
    "To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io import make_backend\n",
    "from granite_io.io import make_io_processor\n",
    "from granite_io.io.base import ChatCompletionInputs\n",
    "from granite_io.io.voting import MajorityVotingProcessor, integer_normalizer\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here.\n",
    "model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "run_server = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(model_name)\n",
    "    server.wait_for_startup(200)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:11434/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_model_name = \"granite3.2:8b\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question with an integral answer\n",
    "completion_inputs = ChatCompletionInputs(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 234651 * 134?\\nAnswer with just a number please.\",\n",
    "        }\n",
    "    ],\n",
    "    thinking=True,\n",
    "    generate_inputs={\"n\": 5, \"temperature\": 0.8, \"max_tokens\": 1024},\n",
    ")\n",
    "completion_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the question through the base model\n",
    "base_processor = make_io_processor(\"Granite 3.3\", backend=backend)\n",
    "results = base_processor.create_chat_completion(completion_inputs)\n",
    "print(\"Outputs from base model:\")\n",
    "for result_num, r in enumerate(results.results):\n",
    "    print(f\"{result_num + 1}: {r.next_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the base model's I/O processor in a majority voting I/O processor.\n",
    "voting_processor = MajorityVotingProcessor(\n",
    "    base_processor, integer_normalizer, samples_per_completion=10\n",
    ")\n",
    "results = voting_processor.create_chat_completion(completion_inputs)\n",
    "print(\"Outputs from base model augmented with majority voting:\")\n",
    "for result_num, r in enumerate(results.results):\n",
    "    print(f\"{result_num + 1}: {r.next_message.content}\")\n",
    "\n",
    "# What's the actual answer?\n",
    "print(f\"---------\\nThe actual answer is: {234651 * 134}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
