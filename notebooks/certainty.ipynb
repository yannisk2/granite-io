{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite certainty intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite certainty intrisic, \n",
    "also known as the [Granite 3.2 8B Instruct Uncertainty LoRA](\n",
    "    https://huggingface.co/ibm-granite/granite-uncertainty-3.2-8b-lora\n",
    ")\n",
    "\n",
    "To run this notebook, you will need to host Granite 3.2 8B and the Granite 3.2 8B \n",
    "Instruct Uncertainty LoRA on your own machine. The constants below assume you started a\n",
    "local vLLM server with the command:\n",
    "```\n",
    "vllm serve ibm-granite/granite-3.2-8b-instruct \\\n",
    "    --enable-lora \\\n",
    "    --max_lora_rank 64 \\\n",
    "    --lora-modules ibm-granite/granite-uncertainty-3.2-8b-lora=ibm-granite/granite-uncertainty-3.2-8b-lora \\\n",
    "    --port 11434 \\\n",
    "    --gpu-memory-utilization 0.5 \\\n",
    "    --max-model-len 8192\n",
    "```\n",
    "\n",
    "Update the constants below to reflecthow you are hosting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    _Granite3Point2Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.certainty import CertaintyIOProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "lora_model_name = \"ibm-granite/granite-uncertainty-3.2-8b-lora\"\n",
    "\n",
    "# You will need to set the following variables to appropriate values for your own\n",
    "# OpenAI-compatible inference server:\n",
    "openai_base_url = \"http://localhost:11434/v1\"\n",
    "openai_base_model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "openai_lora_model_name = \"ibm-granite/granite-uncertainty-3.2-8b-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = make_backend(\n",
    "    \"openai\",\n",
    "    {\n",
    "        \"model_name\": openai_base_model_name,\n",
    "        \"openai_base_url\": openai_base_url,\n",
    "    },\n",
    ")\n",
    "lora_backend = make_backend(\n",
    "    \"openai\",\n",
    "    {\n",
    "        \"model_name\": openai_lora_model_name,\n",
    "        \"openai_base_url\": openai_base_url,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = _Granite3Point2Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which of my pets have fleas?\"},\n",
    "        ],\n",
    "        \"documents\": [\n",
    "            {\"text\": \"My dog has fleas.\"},\n",
    "            {\"text\": \"My cat does not have fleas.\"},\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example input through Granite 3.2 to get an answer\n",
    "granite_io_proc = make_io_processor(\"Granite 3.2\", backend=backend)\n",
    "result = await granite_io_proc.acreate_chat_completion(chat_input)\n",
    "result.results[0].next_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the model's output to the chat\n",
    "next_chat_input = chat_input.with_next_message(result.results[0].next_message)\n",
    "next_chat_input.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the I/O processor for the certainty intrinsic\n",
    "io_proc = CertaintyIOProcessor(lora_backend)\n",
    "\n",
    "# Set temperature to 0 because we are not sampling from the intrinsic's output\n",
    "next_chat_input = next_chat_input.with_addl_generate_params({\"temperature\": 0.0})\n",
    "\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(next_chat_input)\n",
    "\n",
    "print(\n",
    "    f\"Certainty score for this response is \"\n",
    "    f\"{chat_result.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with an artifical poor-quality assistant response.\n",
    "from granite_io.types import AssistantMessage\n",
    "\n",
    "chat_result_2 = await io_proc.acreate_chat_completion(\n",
    "    chat_input.with_next_message(\n",
    "        AssistantMessage(content=\"Your iguana is absolutely covered in fleas.\")\n",
    "    ).with_addl_generate_params({\"temperature\": 0.0})\n",
    ")\n",
    "print(\n",
    "    f\"Certainty score for this response is \"\n",
    "    f\"{chat_result_2.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The certainty intrinsic can also be run before the assistant responds.\n",
    "# In this case, the intrinsic estimates what will be the certainty of the model's top-1\n",
    "# response.\n",
    "chat_result_3 = await io_proc.acreate_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"temperature\": 0.0})\n",
    ")\n",
    "print(\n",
    "    f\"Certainty score for this response is \"\n",
    "    f\"{chat_result_3.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
