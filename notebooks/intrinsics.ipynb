{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the granite-io and the Granite RAG Intrinsics\n",
    "\n",
    "This notebook provides a high-level introduction to the `granite-io` library and to\n",
    "the [Granite RAG intrinsics](https://arxiv.org/html/2504.11704v1).\n",
    "\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notebooks in this directory provide a more in-depth treatment of concepts covered\n",
    "in this notebook:\n",
    "* \n",
    "* Advanced end-to-end Retrieval Augmented Generation flows: [rag.ipynb](./rag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    Granite3Point2Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.base import RewriteRequestProcessor\n",
    "from granite_io.io.retrieval.util import download_mtrag_embeddings\n",
    "from granite_io.io.retrieval import (\n",
    "    Retriever,\n",
    "    InMemoryRetriever,\n",
    "    RetrievalRequestProcessor,\n",
    ")\n",
    "from granite_io.io.answerability import (\n",
    "    AnswerabilityIOProcessor,\n",
    ")\n",
    "from granite_io.io.query_rewrite import QueryRewriteIOProcessor\n",
    "from granite_io.io.citations import CitationsIOProcessor, CitationsCompositeIOProcessor\n",
    "from granite_io.io.hallucinations import HallucinationsIOProcessor\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.certainty import CertaintyIOProcessor\n",
    "from granite_io.types import GenerateInputs\n",
    "from granite_io.visualization import CitationsWidget\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "corpus_name = \"govt\"\n",
    "embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embeds.parquet\"\n",
    "embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "query_rewrite_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-query-rewrite\"\n",
    "citations_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-citation-generation\"\n",
    "answerability_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction\"\n",
    "hallucination_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-hallucination-detection\"\n",
    "certainty_lora_name = \"ibm-granite/granite-uncertainty-3.2-8b-lora\"\n",
    "\n",
    "# Download the indexed corpus if it hasn't already been downloaded.\n",
    "# This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "if not os.path.exists(embeddings_location):\n",
    "    download_mtrag_embeddings(embedding_model_name, corpus_name, embeddings_location)\n",
    "\n",
    "run_server = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## granite-io\n",
    "\n",
    "The `granite-io` library provides input and output processing for large language models.\n",
    "In this context, *input and output processing* refers to the steps that happen \n",
    "immediately before and after low-level model inference. These steps include:\n",
    "\n",
    "* **Input processing:** Translating application data structures such as messages and \n",
    "  documents into a string prompt for a particular model\n",
    "* **Output processing:** Parsing the raw string output of a language model into \n",
    "  structured application data\n",
    "* **Constrained decoding:** Constraining the raw string output of an LLM to ensure that\n",
    "  the model's output will always parse into structured application data\n",
    "* **Inference-time scaling:** Extracting a higher-quality answer from an LLM by \n",
    "  combining the results of multiple inference calls.\n",
    "\n",
    "\n",
    "`granite-io` includes three main types of entry points:\n",
    "* **Backend connectors** connect the `granite-io` library to different model inference \n",
    "  engines and vector databases.\n",
    "  The other components of `granite-io` use these adapters to invoke model inference with\n",
    "  exactly the right low-level parameters for each model and inference layer.\n",
    "* **InputOutputProcessors** provide input and output processing for specific models.\n",
    "  An InputOutputProcessor exposes a \"chat completions\" interface, where the input is the\n",
    "  structured representation of a conversation and the output is the next turn of the\n",
    "  conversation.\n",
    "  For some models, such as [IBM Granite 3.3](https://huggingface.co/collections/\n",
    "  ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3), we also provide\n",
    "  separate APIs that only perform input processing or output processing.\n",
    "* **RequestProcessors** rewrite chat completion requests in various ways, such as \n",
    "  rewording messages, attaching RAG documents, or filtering documents. You can chain\n",
    "  one or more RequestProcessors with an InputOutputProcessor to implement a custom \n",
    "  inference workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backends\n",
    "\n",
    "All the parts of `granite-io` that we exercise in this notebook rely on the Backend \n",
    "API, so we start by instantiating a Backend instance for each of the models that\n",
    "this notebook uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        model_name,\n",
    "        lora_adapters=[\n",
    "            (lora_name, lora_name)\n",
    "            for lora_name in (\n",
    "                query_rewrite_lora_name,\n",
    "                citations_lora_name,\n",
    "                answerability_lora_name,\n",
    "                hallucination_lora_name,\n",
    "                certainty_lora_name,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    query_rewrite_lora_backend = server.make_lora_backend(query_rewrite_lora_name)\n",
    "    citations_lora_backend = server.make_lora_backend(citations_lora_name)\n",
    "    answerability_lora_backend = server.make_lora_backend(answerability_lora_name)\n",
    "    hallucination_lora_backend = server.make_lora_backend(hallucination_lora_name)\n",
    "    certainty_lora_backend = server.make_lora_backend(certainty_lora_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    query_rewrite_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": query_rewrite_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    citations_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": citations_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    answerability_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": answerability_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    hallucination_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": hallucination_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    certainty_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": certainty_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Backend API in `granite-io` runs low-level inference on the target\n",
    "model, passing in raw string prompts and inference paramters and receiving back raw \n",
    "string results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_result = await backend.generate(\n",
    "    GenerateInputs(\n",
    "        prompt=\"Complete this sequence: 2, 3, 5, 7, 11, 13, \",\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=12,\n",
    "    )\n",
    ")\n",
    "print(generate_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users don't interact with the low-level backend API directly. The recommended way\n",
    "to use `granite-io` is via the InputOutputProcessor APIs, which convert high-level \n",
    "request into the specific combination of inference paramters that the model needs,\n",
    "run inference, and then convert the model's raw output into something that an \n",
    "application can use directly.\n",
    "\n",
    "Let's create an example chat completion request so we can show how the high-level \n",
    "InputOutputProcessor API works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = Granite3Point2Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the City of Dublin, CA help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hi there. Can you answer questions about fences?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Absolutely, I can provide general information about \"\n",
    "                \"fences in Dublin, CA.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Great. I want to add one in my front yard. Do I need a \"\n",
    "                \"permit?\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def print_chat(c):\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"\\n\".join([f\"**{m.role.capitalize()}:** {m.content}\\n\" for m in c.messages])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print_chat(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chat completion request models a scenario where the user is talking to the \n",
    "automated help desk for the City of Dublin, CA and has just asked a question about \n",
    "permitting for installing fences. Running this chat completion request should produce\n",
    "an assistant response to this question.\n",
    "\n",
    "If we pass our chat completion (`chat_input`) to a `granite-io` InputOutputProcessor's \n",
    "`create_chat_completion()` method, the InputOutputProcessor will create a string prompt\n",
    "for the model, set up model-specific generation parameters, invoke model inference, and\n",
    "parse the model's raw output into a structured message.\n",
    "\n",
    "Here we create an InputOutputProcessor for the [IBM Granite 3.2](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-instruct) model and point that InputOutputProcessor at the backend we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "# Use the IO processor to generate a chat completion\n",
    "non_rag_result = io_proc.create_chat_completion(chat_input)\n",
    "display(Markdown(non_rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's response here is generic and vague, because the model's training data does \n",
    "not cover obscure zoning ordinances of small cities in northern California.\n",
    "\n",
    "We can use the \n",
    "[Granite 3.2 8b Instruct - Uncertainty](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-uncertainty)\n",
    "model to flag cases such as this one that are not covered by the base model's \n",
    "training data. \n",
    "\n",
    "This model comes packaged as a LoRA adapter on top of Granite 3.2. To run the model, we\n",
    "create an instance of `CertaintyIOProcessor` -- the `granite-io` InputOutputProcessor\n",
    "for this model -- and point this InputOutputProcessor at a Backend that we have\n",
    "connected to the model's LoRA adapter. Then we can pass the same chat completion request\n",
    "into the model to compute a certainty score from 0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certainty_io_proc = CertaintyIOProcessor(certainty_lora_backend)\n",
    "certainty_score = (\n",
    "    certainty_io_proc.create_chat_completion(chat_input).results[0].next_message.content\n",
    ")\n",
    "print(f\"Certainty score is {certainty_score} out of 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low certainty score indicates that the model's training data does not align closely\n",
    "with this question.\n",
    "\n",
    "To answer this question properly, we need to provide the model with domain-specific \n",
    "information. One of the most popular ways to add domain-specific information to an LLM\n",
    "is to use the Retrieval-Augmented Generation (RAG) pattern. RAG involves retrieving\n",
    "snippets of text from a collection of documents and adding those snippets to the model's\n",
    "prompt.\n",
    "\n",
    "\n",
    "In this case, the relevant information can be found in the Government \n",
    "corpus of the [MTRAG multi-turn RAG benchmark](https://github.com/IBM/mt-rag-benchmark).\n",
    "Similar to its connectors for inference backends, `granite-io` has adapters for \n",
    "RAG retrieval backends.\n",
    "\n",
    "Let's spin up a connection in-memory vector database, using embeddings that we've \n",
    "precomputed offline from the MTRAG Government corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`granite-io` also includes a RequestProcessor that performs the retrieval phase of\n",
    "RAG. This class, called `RetrievalRequestProcessor`, takes as input a chat completion\n",
    "request. The RequestProcessor uses the text of the last user turn to query a `Retriever`\n",
    "instance and fetch document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "chat_input_with_docs = retrieval_request_proc.process(chat_input)[0]\n",
    "{\n",
    "    k: v\n",
    "    for k, v in chat_input_with_docs.model_dump().items()\n",
    "    if k in (\"messages\", \"documents\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the last user turn in this conversation is:\n",
    "> **User:** Great. I want to add one in my front yard. Do I need a permit?\n",
    "\n",
    "This text is missing key details for retrieving relevant documents: What does the \n",
    "user want to add to their front yard, and what city's municipal code applies to this\n",
    "yard? As a result, the retrieved documents aren't actually relevant to the user's \n",
    "question.\n",
    "\n",
    "The [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction)\n",
    "provides a robust way to detect this kind of problem. Here's what happens if we \n",
    "run the chat completion request with irrelevant document snippets through the \n",
    "answerability model, using the\n",
    "`granite_io` IO processor for the model to handle input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval step from before...\n",
    "retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "chat_input_with_docs = retrieval_request_proc.process(chat_input)[0]\n",
    "\n",
    "# ...followed by an answerability check\n",
    "answerability_proc = AnswerabilityIOProcessor(answerability_lora_backend)\n",
    "answerability_proc.create_chat_completion(chat_input_with_docs).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-query-rewrite) to rewrite\n",
    "the last user turn into a string that is more useful for retrieiving document snippets.\n",
    "`granite-io` includes an InputOutputProcessor for running this model.\n",
    "Here's how to use this InputOutputProcessor to apply this model to our example \n",
    "conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_io_proc = QueryRewriteIOProcessor(query_rewrite_lora_backend)\n",
    "rewrite_io_proc.create_chat_completion(chat_input).results[0].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query rewrite model turns the last user turn in this conversation from:\n",
    "> **User:** Great. I want to add one in my front yard. Do I need a permit?\n",
    "\n",
    "...to a version of the same question that includes vital additional context:\n",
    "> **User:** Do I need a permit to add a fence in my front yard in Dublin, CA?\n",
    "\n",
    "This more specific query should allow the retriever to fetch better document snippets.\n",
    "\n",
    "We can use the  LoRA Adapter for Answerability Classification that we showed earlier \n",
    "to validate that the retrieved data is indeed relevant to the user's question.\n",
    "The following code snippet uses `granite-io` APIs to rewrite the user query, \n",
    "fetch relevant document snippets, and check for answerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo initialization so this cell can run independently of previous cells\n",
    "rewrite_io_proc = QueryRewriteIOProcessor(query_rewrite_lora_backend)\n",
    "rewrite_request_proc = RewriteRequestProcessor(rewrite_io_proc)\n",
    "retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "answerability_proc = AnswerabilityIOProcessor(answerability_lora_backend)\n",
    "\n",
    "\n",
    "# Rewrite the last user turn into something more suitable for retrieval.\n",
    "input = rewrite_request_proc.process(chat_input)[0]\n",
    "\n",
    "# Retrieve document snippets based on the rewritten turn and attach them to the chat\n",
    "# completion request.\n",
    "input = retrieval_request_proc.process(input)[0]\n",
    "input = input.with_messages(chat_input.messages)\n",
    "\n",
    "answerability_proc.create_chat_completion(input).results[0].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching relevant information causes the model to respond with a more specific and \n",
    "detailed answer. Here's the result that we get when we pass the rewritten chat \n",
    "completion request to the InputOutputProcessor for Granite 3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "rag_result = io_proc.create_chat_completion(input)\n",
    "display(Markdown(rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer contains specific details about permits for building fences in Dublin, CA.\n",
    "These facts should grounded in documents retrieved from the corpus. We would like\n",
    "to be able to prove that the model used the data from the corpus and did not \n",
    "hallucinate a fictitious building code.\n",
    "\n",
    "We can use the [LoRA Adapter for Citation Generation](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-citation-generation\n",
    ") to explain exactly how this response is grounded in the documents that the rewritten\n",
    "user query retrieves. As with the other models we've shown so far, `granite-io` includes\n",
    "an InputOutputProcessor for this model. We can use this InputOutputProcessor to add\n",
    "citations to the assistant response from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_io_proc = CitationsIOProcessor(citations_lora_backend)\n",
    "\n",
    "# Add the assistant response to the original chat completion request\n",
    "input_with_next_message = input.with_next_message(rag_result.results[0].next_message)\n",
    "\n",
    "# Augment this response with citations to the RAG document snippets\n",
    "results_with_citations = citations_io_proc.create_chat_completion(\n",
    "    input_with_next_message\n",
    ")\n",
    "CitationsWidget().show(input_with_next_message, results_with_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [LoRA Adapter for Hallucination Detection in RAG outputs](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-hallucination-detection\n",
    ") to check whether each sentence of the assistant response is consistent with the\n",
    "information in the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinations_io_proc = HallucinationsIOProcessor(hallucination_lora_backend)\n",
    "result_with_hallucinations = hallucinations_io_proc.create_chat_completion(\n",
    "    input_with_next_message\n",
    ").results[0]\n",
    "\n",
    "print(\"Hallucination Checks:\")\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [h.model_dump() for h in result_with_hallucinations.next_message.hallucinations]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `granite-io` library also allows developers to create their own custom \n",
    "InputOutputProcessors. For example, here's an InputOutputProcessor that rolls up the\n",
    "rewrite, retrieval, and citations processing steps from this notebook into a single\n",
    "`create_chat_completion()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.base import InputOutputProcessor\n",
    "from granite_io.backend import Backend\n",
    "from granite_io.io.base import ChatCompletionInputs, ChatCompletionResults\n",
    "\n",
    "\n",
    "class MyRAGIOProcessor(InputOutputProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_backend: Backend,\n",
    "        base_model_name: str,\n",
    "        retriever: Retriever,\n",
    "        query_rewrite_lora_backend: Backend,\n",
    "        citations_lora_backend: Backend,\n",
    "    ):\n",
    "        self.rewrite_request_proc = RewriteRequestProcessor(\n",
    "            QueryRewriteIOProcessor(query_rewrite_lora_backend)\n",
    "        )\n",
    "        self.retrieval_request_proc = RetrievalRequestProcessor(retriever)\n",
    "\n",
    "        # Build up a chain of two IO processors: base model -> citations\n",
    "        self.io_proc_chain = CitationsCompositeIOProcessor(\n",
    "            make_io_processor(base_model_name, backend=base_backend),\n",
    "            citations_lora_backend,\n",
    "        )\n",
    "\n",
    "    async def acreate_chat_completion(\n",
    "        self, inputs: ChatCompletionInputs\n",
    "    ) -> ChatCompletionResults:\n",
    "        \"\"\"\n",
    "        Chat completions API inherited from the ``InputOutputProcessor`` base class.\n",
    "\n",
    "        :param inputs: Structured representation of the inputs to a chat completion\n",
    "            request, possibly including additional fields that only this input-output\n",
    "            processor can consume\n",
    "\n",
    "        :returns: The next message that the model produces when fed the specified\n",
    "            inputs, plus additional information about the low-level request.\n",
    "        \"\"\"\n",
    "        original_inputs = inputs\n",
    "\n",
    "        # Rewrite the last user turn for retrieval\n",
    "        inputs = (await rewrite_request_proc.aprocess(inputs))[0]\n",
    "\n",
    "        # Retrieve documents with the rewritten last turn\n",
    "        inputs = (await retrieval_request_proc.aprocess(inputs))[0]\n",
    "\n",
    "        # Switch back to original version of last turn\n",
    "        inputs = inputs.with_messages(original_inputs.messages)\n",
    "\n",
    "        # Generate a response and add citations\n",
    "        return await self.io_proc_chain.acreate_chat_completion(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap all of the functionality we've shown so far in a single class that \n",
    "inherits from the `InputOutputProcessor` interface in `granite-io`. Packaging things\n",
    "this way lets applications treat this multi-step flow as if it was a single chat \n",
    "completion request to a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_io_proc = MyRAGIOProcessor(\n",
    "    base_backend=backend,\n",
    "    base_model_name=model_name,\n",
    "    retriever=retriever,\n",
    "    query_rewrite_lora_backend=query_rewrite_lora_backend,\n",
    "    citations_lora_backend=citations_lora_backend,\n",
    ")\n",
    "\n",
    "rag_results = rag_io_proc.create_chat_completion(chat_input)\n",
    "CitationsWidget().show(input_with_next_message, rag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
